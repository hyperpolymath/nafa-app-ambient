#!/usr/bin/env node
// SPDX-License-Identifier: MIT OR AGPL-3.0-or-later
// SPDX-FileCopyrightText: 2025 hyperpolymath

/**
 * NAFA MVP CLI Demo (Node.js compatible)
 *
 * Runs the full journey + annotation flow in the terminal
 * to demonstrate the MVP functionality without needing a browser.
 *
 * Run with: node server/src/demo-node.js
 */

// Domain helpers (matching ReScript types)
const sensoryLevelDescription = (level) => {
  if (level <= 2) return "Very Low";
  if (level <= 4) return "Low";
  if (level <= 6) return "Moderate";
  if (level <= 8) return "High";
  return "Very High";
};

const transportModeEmoji = {
  Walk: "ğŸš¶",
  Bus: "ğŸšŒ",
  Train: "ğŸš†",
  Tram: "ğŸšŠ",
  Metro: "ğŸš‡",
};

const renderSensoryLevel = (label, level) => {
  const filled = "â–ˆ".repeat(level);
  const empty = "â–‘".repeat(10 - level);
  return `${label}: ${filled}${empty} (${level}/10 - ${sensoryLevelDescription(level)})`;
};

// Sample journey data
const journey = {
  id: "journey-001",
  title: "Morning Commute to Central Library",
  status: "Planning",
  estimatedMinutes: 35,
  segments: [
    {
      id: "seg-1",
      transportMode: "Walk",
      fromLocation: "Home",
      toLocation: "Oak Street Bus Stop",
      durationMinutes: 5,
      sensoryWarning: null,
      sensoryLevels: { noise: 3, light: 5, crowd: 2 },
    },
    {
      id: "seg-2",
      transportMode: "Bus",
      fromLocation: "Oak Street Bus Stop",
      toLocation: "City Center Station",
      durationMinutes: 15,
      sensoryWarning: "Rush hour: expect moderate crowding",
      sensoryLevels: { noise: 6, light: 4, crowd: 7 },
    },
    {
      id: "seg-3",
      transportMode: "Walk",
      fromLocation: "City Center Station",
      toLocation: "Central Library",
      durationMinutes: 8,
      sensoryWarning: "Construction noise on Main Street",
      sensoryLevels: { noise: 8, light: 6, crowd: 5 },
    },
  ],
  sensoryAnnotations: [
    {
      id: "ann-1",
      locationId: "city-center-station",
      locationName: "City Center Station",
      noise: 7,
      light: 5,
      crowd: 8,
      notes: "Very busy during morning rush, quieter after 9am",
      timestamp: Date.now(),
    },
  ],
};

// Demo flow
console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              NAFA MVP DEMO - Journey + Annotations           â•‘
â•‘     Neurodiverse App for Adventurers                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This demo shows the two core MVP features:
  1. Journey Plan View - with sensory-aware route segments
  2. Sensory Annotation Flow - crowdsourced location data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

`);

// Wait function for pacing
const wait = (ms) => new Promise((resolve) => setTimeout(resolve, ms));

async function runDemo() {
  // Step 1: Show Journey Plan
  console.log("ğŸ“ STEP 1: Journey Plan View\n");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log(`ğŸ—ºï¸  JOURNEY: ${journey.title}`);
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log(`Status: ğŸ“‹ ${journey.status}`);
  console.log(`Estimated Time: ${journey.estimatedMinutes} minutes\n`);
  console.log("ROUTE SEGMENTS:\n");

  for (const segment of journey.segments) {
    const emoji = transportModeEmoji[segment.transportMode];
    console.log(`  ${emoji} ${segment.transportMode}`);
    console.log("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    console.log(`  From: ${segment.fromLocation}`);
    console.log(`  To:   ${segment.toLocation}`);
    console.log(`  Duration: ${segment.durationMinutes} minutes`);

    if (segment.sensoryWarning) {
      console.log(`    âš ï¸  ${segment.sensoryWarning}`);
    }

    if (segment.sensoryLevels) {
      console.log("    Sensory Levels:");
      console.log(`      ${renderSensoryLevel("Noise", segment.sensoryLevels.noise)}`);
      console.log(`      ${renderSensoryLevel("Light", segment.sensoryLevels.light)}`);
      console.log(`      ${renderSensoryLevel("Crowd", segment.sensoryLevels.crowd)}`);
    }
    console.log("");
  }

  await wait(300);

  // Step 2: Show existing annotations
  console.log("\nğŸ“ SENSORY ANNOTATIONS (Community Contributed)");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

  for (const ann of journey.sensoryAnnotations) {
    console.log(`  ğŸ“ ${ann.locationName}`);
    console.log(`      Noise: ${ann.noise}/10 | Light: ${ann.light}/10 | Crowd: ${ann.crowd}/10`);
    if (ann.notes) {
      console.log(`      Notes: ${ann.notes}`);
    }
    console.log("");
  }

  await wait(300);

  // Step 3: Show annotation flow
  console.log("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log("ğŸ“ STEP 2: Sensory Annotation Flow\n");
  console.log("Demonstrating how users contribute sensory data...\n");

  const annotationForm = {
    locationId: "oak-street-stop",
    locationName: "Oak Street Bus Stop",
    noise: 4,
    light: 6,
    crowd: 3,
    notes: "",
  };

  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log("ğŸ“ SENSORY ANNOTATION");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log(`Location: ${annotationForm.locationName}\n`);
  console.log("Rate the sensory environment at this location:\n");
  console.log(`ğŸ”Š ${renderSensoryLevel("Noise Level", annotationForm.noise)}`);
  console.log("   (0 = Silent, 10 = Very Loud)\n");
  console.log(`ğŸ’¡ ${renderSensoryLevel("Light Level", annotationForm.light)}`);
  console.log("   (0 = Very Dark, 10 = Very Bright)\n");
  console.log(`ğŸ‘¥ ${renderSensoryLevel("Crowd Level", annotationForm.crowd)}`);
  console.log("   (0 = Empty, 10 = Very Crowded)\n");

  await wait(300);

  // Simulate user adjusting values
  console.log("\n[User adjusts: Noise +1, adds notes]\n");
  annotationForm.noise = 5;
  annotationForm.notes = "Sheltered stop, moderate traffic noise from street";

  console.log(`ğŸ”Š ${renderSensoryLevel("Noise Level", annotationForm.noise)}`);
  console.log(`ğŸ“ Notes: ${annotationForm.notes}\n`);

  await wait(300);

  // Submit
  console.log("[User submits annotation]\n");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log("âœ… ANNOTATION SAVED");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
  console.log(`Thank you for contributing sensory data for:`);
  console.log(`ğŸ“ ${annotationForm.locationName}\n`);
  console.log("Your annotation helps other neurodiverse travelers");
  console.log("prepare for their journeys.\n");

  // Summary
  console.log("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log("MVP DEMO COMPLETE");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
  console.log("The NAFA MVP demonstrates:");
  console.log("  âœ“ Journey planning with sensory-aware route segments");
  console.log("  âœ“ Sensory annotation collection for locations");
  console.log("  âœ“ TEA (The Elm Architecture) pattern in ReScript");
  console.log("  âœ“ Deno/Node server with REST API\n");
  console.log("Run commands:");
  console.log("  just mvp-demo      - Run this demo");
  console.log("  just mvp-server    - Start the API server");
  console.log("  just mvp-build     - Build ReScript modules\n");
}

runDemo();
